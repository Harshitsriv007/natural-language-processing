{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN Neighbors Text Classifier Final.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumitdua10/natural-language-processing/blob/master/KNN_Neighbors_Text_Classifier_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "lLmxOcTFn5UZ",
        "colab_type": "code",
        "outputId": "9385086f-47fc-451e-c561-c6fb74bd520c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import genesis\n",
        "nltk.download('genesis')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "genesis_ic = wn.ic(genesis, False, 0.0)\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/genesis.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1mE4xYUhW_7Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class KNN_NLC_Classifer():\n",
        "    def __init__(self, k=1, distance_type = 'path'):\n",
        "        self.k = k\n",
        "        self.distance_type = distance_type\n",
        "\n",
        "    # This function is used for training\n",
        "    def fit(self, x_train, y_train):\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "\n",
        "    # This function runs the K(1) nearest neighbour algorithm and\n",
        "    # returns the label with closest match. \n",
        "    def predict(self, x_test):\n",
        "        self.x_test = x_test\n",
        "        y_predict = []\n",
        "\n",
        "        for i in range(len(x_test)):\n",
        "            max_sim = 0\n",
        "            max_index = 0\n",
        "            for j in range(self.x_train.shape[0]):\n",
        "                temp = self.document_similarity(x_test[i], self.x_train[j])\n",
        "                if temp > max_sim:\n",
        "                    max_sim = temp\n",
        "                    max_index = j\n",
        "            y_predict.append(self.y_train[max_index])\n",
        "        return y_predict\n",
        "\n",
        "\n",
        "    def convert_tag(self, tag):\n",
        "        \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
        "        tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
        "        try:\n",
        "            return tag_dict[tag[0]]\n",
        "        except KeyError:\n",
        "            return None\n",
        "\n",
        "\n",
        "    def doc_to_synsets(self, doc):\n",
        "        \"\"\"\n",
        "            Returns a list of synsets in document.\n",
        "            Tokenizes and tags the words in the document doc.\n",
        "            Then finds the first synset for each word/tag combination.\n",
        "        If a synset is not found for that combination it is skipped.\n",
        "\n",
        "        Args:\n",
        "            doc: string to be converted\n",
        "\n",
        "        Returns:\n",
        "            list of synsets\n",
        "        \"\"\"\n",
        "        tokens = word_tokenize(doc+' ')\n",
        "        \n",
        "        l = []\n",
        "        tags = nltk.pos_tag([tokens[0] + ' ']) if len(tokens) == 1 else nltk.pos_tag(tokens)\n",
        "        \n",
        "        for token, tag in zip(tokens, tags):\n",
        "            syntag = self.convert_tag(tag[1])\n",
        "            syns = wn.synsets(token, syntag)\n",
        "            if (len(syns) > 0):\n",
        "                l.append(syns[0])\n",
        "        return l  \n",
        "    \n",
        "\n",
        "    def similarity_score(self, s1, s2, distance_type = 'path'):\n",
        "          \"\"\"\n",
        "          Calculate the normalized similarity score of s1 onto s2\n",
        "          For each synset in s1, finds the synset in s2 with the largest similarity value.\n",
        "          Sum of all of the largest similarity values and normalize this value by dividing it by the\n",
        "          number of largest similarity values found.\n",
        "\n",
        "          Args:\n",
        "              s1, s2: list of synsets from doc_to_synsets\n",
        "\n",
        "          Returns:\n",
        "              normalized similarity score of s1 onto s2\n",
        "          \"\"\"\n",
        "          s1_largest_scores = []\n",
        "\n",
        "          for i, s1_synset in enumerate(s1, 0):\n",
        "              max_score = 0\n",
        "              for s2_synset in s2:\n",
        "                  if distance_type == 'path':\n",
        "                      score = s1_synset.path_similarity(s2_synset, simulate_root = False)\n",
        "                  else:\n",
        "                      score = s1_synset.wup_similarity(s2_synset)                  \n",
        "                  \n",
        "                      #score = s1_synset.jcn_similarity(s2_synset, genesis_ic)\n",
        "                      #score = s1_synset.jcn_similarity(s2_synset, brown_ic)\n",
        "                      #score = s1_synset.lin_similarity(s2_synset, semcor_ic)\n",
        "                      #score = s1_synset.res_similarity(s2_synset, genesis_ic)\n",
        "                      #score = s1_synset.res_similarity(s2_synset, brown_ic)\n",
        "                      #score = s1_synset.wup_similarity(s2_synset)\n",
        "                      #score = s1_synset.lch_similarity(s2_synset)\n",
        "                      #score = s1_synset.wup_similarity(s2_synset)\n",
        "\n",
        "                  if score != None:\n",
        "                      if score > max_score:\n",
        "                          max_score = score\n",
        "              \n",
        "              if max_score != 0:\n",
        "                  s1_largest_scores.append(max_score)\n",
        "          \n",
        "          mean_score = np.mean(s1_largest_scores)\n",
        "                 \n",
        "          return mean_score  \n",
        "        \n",
        "        \n",
        "    def document_similarity(self,doc1, doc2):\n",
        "          \"\"\"Finds the symmetrical similarity between doc1 and doc2\"\"\"\n",
        "\n",
        "          synsets1 = self.doc_to_synsets(doc1)\n",
        "          synsets2 = self.doc_to_synsets(doc2)\n",
        "          \n",
        "          return (self.similarity_score(synsets1, synsets2) + self.similarity_score(synsets2, synsets1)) / 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "38Ex1R01xXyd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0afc3913-deee-4c84-a748-89f5d7a3b60c"
      },
      "cell_type": "code",
      "source": [
        "doc1 = 'I like rains'\n",
        "doc2 = 'I like showers'\n",
        "x = KNN_NLC_Classifer()\n",
        "print(\"Test Similarity Score: \", x.document_similarity(doc1, doc2))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Similarity Score:  0.6946386946386947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R7P_3LkmoDzG",
        "colab_type": "code",
        "outputId": "fe0958c9-cba0-452d-ebb4-f66193b8ea56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "synonyms = []\n",
        "antonyms = []\n",
        "#print(wordnet.synsets(\"shower\"))\n",
        "for syn in wordnet.synsets(\"shower\"):\n",
        "    #print(syn)\n",
        "    for l in syn.lemmas():\n",
        "        synonyms.append(l.name())\n",
        "\n",
        "print(synonyms)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['shower', 'shower', 'shower_bath', 'shower', 'rain_shower', 'shower', 'cascade', 'exhibitor', 'exhibitioner', 'shower', 'shower', 'lavish', 'shower', 'shower', 'shower', 'shower', 'shower_down', 'shower']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6H5oUQPLp1Jl",
        "colab_type": "code",
        "outputId": "5c369efd-f302-48f3-abd1-7b343f9643c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "# 1. Importing the dataset\n",
        "#we'll use the demo dataset available at Watson NLC Classifier Demo.\n",
        "FILENAME = \"https://raw.githubusercontent.com/watson-developer-cloud/natural-language-classifier-nodejs/master/training/weather_data_train.csv\"          \n",
        "\n",
        "dataset = pd.read_csv(FILENAME, header = None)\n",
        "\n",
        "dataset.rename(columns = {0:'text', 1:'answer'}, inplace = True)\n",
        "\n",
        "dataset['output'] = np.where(dataset['answer'] == 'temperature', 1,0)\n",
        "Num_Words = dataset.shape[0]\n",
        "\n",
        "print(dataset.head())\n",
        "print(\"\\nSize of input file is \", dataset.shape)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                            text       answer  output\n",
            "0           How hot is it today?  temperature       1\n",
            "1             Is it hot outside?  temperature       1\n",
            "2  Will it be uncomfortably hot?  temperature       1\n",
            "3         Will it be sweltering?  temperature       1\n",
            "4          How cold is it today?  temperature       1\n",
            "\n",
            "Size of input file is  (50, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JIOtINE-q_Er",
        "colab_type": "code",
        "outputId": "d5112a1e-4e85-4f39-80c9-0378ac8b0f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "nltk.download('stopwords')\n",
        "s = stopwords.words('english')\n",
        "#add additional stop words\n",
        "s.extend(['today', 'tomorrow', 'outside', 'out', 'there'])\n",
        "\n",
        "ps = nltk.wordnet.WordNetLemmatizer()\n",
        "for i in range(dataset.shape[0]):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', dataset.loc[i,'text'])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "\n",
        "    review = [ps.lemmatize(word) for word in review if not word in s]\n",
        "    review = ' '.join(review)\n",
        "    dataset.loc[i, 'text'] = review\n",
        "\n",
        "X_train = dataset['text']\n",
        "y_train = dataset['output']\n",
        "\n",
        "print(\"Below is the sample of training text after removing the stop words\")\n",
        "print(dataset['text'][:10])\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'today', 'tomorrow', 'outside', 'out', 'there']\n",
            "Below is the sample of training text after removing the stop words\n",
            "0                     hot\n",
            "1                     hot\n",
            "2       uncomfortably hot\n",
            "3              sweltering\n",
            "4                    cold\n",
            "5                    cold\n",
            "6      uncomfortably cold\n",
            "7                  frigid\n",
            "8           expected high\n",
            "9    expected temperature\n",
            "Name: text, dtype: object\n",
            "(50, 46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1Zg-ufgh3XFb",
        "colab_type": "code",
        "outputId": "ac6ee39c-8698-49db-94db-c5fc7927fdbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# 4. Train the Classifier\n",
        "classifier = KNN_NLC_Classifer(k=1, distance_type='path')\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "final_test_list = ['will it rain', 'Is it hot outside?' , 'What is the expected high for today?' , \n",
        "                   'Will it be foggy tomorrow?', 'Should I prepare for sleet?',\n",
        "                     'Will there be a storm today?', 'do we need to take umbrella today',\n",
        "                    'will it be wet tomorrow', 'is it humid tomorrow', 'what is the precipitation today',\n",
        "                    'is it freezing outside', 'is it cool outside', \"are there strong winds outside\",]\n",
        "                 \n",
        "test_corpus = []\n",
        "for i in range(len(final_test_list)):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', final_test_list[i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "\n",
        "    review = [ps.lemmatize(word) for word in review if not word in s]\n",
        "    review = ' '.join(review)\n",
        "    test_corpus.append(review)\n",
        "\n",
        "y_pred_final = classifier.predict(test_corpus)\n",
        "\n",
        "output_df = pd.DataFrame(data = {'text': final_test_list, 'code': y_pred_final})\n",
        "output_df['answer'] = np.where(output_df['code']==1, 'Temperature','Conditions')\n",
        "print(output_df)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "    code                                  text       answer\n",
            "0      0                          will it rain   Conditions\n",
            "1      1                    Is it hot outside?  Temperature\n",
            "2      1  What is the expected high for today?  Temperature\n",
            "3      1            Will it be foggy tomorrow?  Temperature\n",
            "4      0           Should I prepare for sleet?   Conditions\n",
            "5      0          Will there be a storm today?   Conditions\n",
            "6      0     do we need to take umbrella today   Conditions\n",
            "7      0               will it be wet tomorrow   Conditions\n",
            "8      1                  is it humid tomorrow  Temperature\n",
            "9      1       what is the precipitation today  Temperature\n",
            "10     1                is it freezing outside  Temperature\n",
            "11     1                    is it cool outside  Temperature\n",
            "12     0        are there strong winds outside   Conditions\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b3qapiHutVeR",
        "colab_type": "code",
        "outputId": "f1a2bcd8-c135-498b-a8d9-f3c5c2025705",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1115
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# 7. Transform the test data; Predict the Test Data and Calculate the score\n",
        "X_final_test_vectorized = vect.transform(test_corpus)\n",
        "#y_pred_final = classifier.predict_proba(X_final_test_vectorized)\n",
        "perc_0, perc_1 = zip(*y_pred_final)\n",
        "print(y_pred_final[0])\n",
        "\n",
        "output_dict = {1:'Temprature', 0: 'Condition'}\n",
        "output_df = pd.DataFrame(data = {'text': final_test_list, 'perc_0': perc_0, 'perc_1':perc_1})\n",
        "print(output_df)\n",
        "output_df['answer'] = np.where(output_df['perc_0'] > 0.5, 'Conditions', 'Temperature')\n",
        "output_df['perc'] = np.where(output_df['answer']=='Temperature', output_df['perc_1'] * 100, output_df['perc_0'] * 100)\n",
        "\n",
        "output_df.drop(['perc_0', 'perc_1'], inplace=True, axis=1)\n",
        "print(output_df.head())\n",
        "print(y_pred_final)\n",
        "\n",
        "#score = roc_auc_score(y_test,y_pred_decision )\n",
        "#print(\"Accuracy Score (AUC) is \", score)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Max Score for  rain  is  1.0 .\n",
            " Closest Match is  rain  index 25\n",
            "Max Score for  hot  is  1.0 .\n",
            " Closest Match is  hot  index 0\n",
            "Max Score for  expected high  is  1.0 .\n",
            " Closest Match is  expected high  index 8\n",
            "Max Score for  foggy  is  0 .\n",
            " Closest Match is  hot  index 0\n",
            "Max Score for  prepare sleet  is  0.731578947368421 .\n",
            " Closest Match is  wind dangerous  index 42\n",
            "Max Score for  storm  is  0.8235294117647058 .\n",
            " Closest Match is  wind dangerous  index 42\n",
            "Max Score for  umbrella  is  0.4122807017543859 .\n",
            " Closest Match is  see sun  index 36\n",
            "Max Score for  rain  is  1.0 .\n",
            " Closest Match is  rain  index 25\n",
            "Max Score for  need take umbrella  is  0.49090909090909085 .\n",
            " Closest Match is  expecting sunny condition  index 28\n",
            "Max Score for  wet  is  0.6845238095238095 .\n",
            " Closest Match is  expected humidity  index 47\n",
            "Max Score for  shower  is  0.4632352941176471 .\n",
            " Closest Match is  see sun  index 36\n",
            "Max Score for  humid  is  1.0 .\n",
            " Closest Match is  humid  index 46\n",
            "Max Score for  precipitation  is  0.5904040404040404 .\n",
            " Closest Match is  much rain fall  index 31\n",
            "Max Score for  going windy  is  1.0 .\n",
            " Closest Match is  windy  index 24\n",
            "Max Score for  going shower  is  0.4414936914936915 .\n",
            " Closest Match is  forecast calling snow  index 35\n",
            "Max Score for  going shower  is  0.4414936914936915 .\n",
            " Closest Match is  forecast calling snow  index 35\n",
            "Max Score for  cloud  is  0.75 .\n",
            " Closest Match is  wind dangerous  index 42\n",
            "Max Score for  rainy  is  0 .\n",
            " Closest Match is  hot  index 0\n",
            "Max Score for  freezing  is  0.4 .\n",
            " Closest Match is  wind dangerous  index 42\n",
            "Max Score for  cool  is  0.7142857142857142 .\n",
            " Closest Match is  high temperature dangerous  index 10\n",
            "Max Score for  strong wind  is  1.0 .\n",
            " Closest Match is  wind dangerous  index 42\n",
            "Max Score for  need take sun screen  is  0.5671626984126984 .\n",
            " Closest Match is  see sun  index 36\n",
            "Max Score for  need take sunscreen  is  0.49249639249639243 .\n",
            " Closest Match is  expecting sunny condition  index 28\n",
            "Max Score for  showery  is  0 .\n",
            " Closest Match is  hot  index 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-5fb81f5792a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mX_final_test_vectorized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m#y_pred_final = classifier.predict_proba(X_final_test_vectorized)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mperc_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperc_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my_pred_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: zip argument #1 must support iteration"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "t1tZ2_H93UbB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "47gopBxbpYtY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Cleaning the texts\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "corpus = []\n",
        "s = stopwords.words('english')\n",
        "print(len(s))\n",
        "\n",
        "#print(type(s))\n",
        "s.remove('not')\n",
        "print(len(s))\n",
        "#review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][0])\n",
        "\n",
        "\n",
        "#review = review.lower()\n",
        "#review = review.split()\n",
        "#print(review)\n",
        "\n",
        "#ps = PorterStemmer()\n",
        "#review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "\n",
        "#review = 'a '.join(review)\n",
        "#print(review)\n",
        "\n",
        "\n",
        "\n",
        "#corpus.append(review)\n",
        "#print(corpus)\n",
        "\n",
        "corpus = []\n",
        "for i in range(0, Num_Words):\n",
        "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
        "    #review = review.lower()\n",
        "    #review = review.split()\n",
        "    #ps = PorterStemmer()\n",
        "    #review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    #review = ' '.join(review)\n",
        "    corpus.append(review)\n",
        "\n",
        "print(len(corpus))\n",
        "#print(corpus.size)\n",
        "#print(corpus.shape)\n",
        "#print(corpus[0])\n",
        "#corpus.remove((corpus[0]))\n",
        "\n",
        "# Creating the Bag of Words model\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(ngram_range = (1,3)).fit(corpus)\n",
        "fnames = cv.get_feature_names()\n",
        "print(len(fnames))\n",
        "X = cv.transform(corpus).toarray()\n",
        "print(X.shape)\n",
        "print(type(X))\n",
        "y = dataset.iloc[:, 1].values\n",
        "\n",
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n",
        "\n",
        "# Fitting Naive Bayes to the Training set\n",
        "import sklearn.naive_bayes as nb\n",
        "import sklearn.linear_model as lm\n",
        "\n",
        "#classifier =  lm.SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
        "#classifier = nb.BernoulliNB()\n",
        "classifier = nb.MultinomialNB()\n",
        "#classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "#y_dec = classifier.decision_function(X_test)\n",
        "#y_prob = classifier.predict_proba(X_test)\n",
        "#df = pd.DataFrame(X_test)\n",
        "#df[1:2] = y_prob\n",
        "#print(df.head())\n",
        "#df.to_csv(TestFile)\n",
        "\n",
        "#print(y_prob.shape)\n",
        "print(y_pred.shape)\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "#cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "#inputtext = input(\"Enter the question\")\n",
        "inputtext = \"food was not good\"\n",
        "review = re.sub('[^a-zA-Z]', ' ', inputtext)\n",
        "review = review.lower()\n",
        "#review = review.split()\n",
        "print(review)\n",
        "#review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "#review = ' '.join(review)\n",
        "\n",
        "print(review)\n",
        "\n",
        "\n",
        "x = list()\n",
        "x = [review]\n",
        "print(x)\n",
        "#list.append(review)\n",
        "#corpus.append(review)\n",
        "#print(len(corpus))\n",
        "#print(corpus[-1])\n",
        "#cv = CountVectorizer()#max_features = 1500)\n",
        "#X_test_run = cv.fit_transform(corpus).toarray()\n",
        "\n",
        "x_new = cv.transform(x).toarray()\n",
        "print(x_new.shape)\n",
        "#print(X_test_run.shape)\n",
        "y = classifier.predict(x_new)\n",
        "print(\"Classifier Response is \",y[-1])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}